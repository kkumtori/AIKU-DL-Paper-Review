{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyPM2+mr/atcAYRSqZB3Y/ov"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["해당 노트북은  [Deep-Learning-Paper-Review-and-Practice](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Sequence_to_Sequence_with_LSTM_Tutorial.ipynb) 내용을 바탕으로 작성되었습니다."],"metadata":{"id":"uMHbti_hJM9I"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8rTVkkyyjs_Q","executionInfo":{"status":"ok","timestamp":1711721758678,"user_tz":-540,"elapsed":23732,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"85e65823-7344-4150-b3f2-9e19006fd64c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/AIKU/Github/AIKU-DL-Paper-Review/code_practices/Seq2Seq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhXBToV1juge","executionInfo":{"status":"ok","timestamp":1711721760689,"user_tz":-540,"elapsed":2012,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"c8c3381e-c832-43e1-9005-6fea90886a88"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AIKU/Github/AIKU-DL-Paper-Review/code_practices/Seq2Seq\n"]}]},{"cell_type":"markdown","source":["# Sequece to Sequnce Learning with Neural Networks (NIPS 2014) 실습"],"metadata":{"id":"1dhWnuTXJ2Qv"}},{"cell_type":"markdown","source":["## BLEU Score 계산을 위한 라이브러리"],"metadata":{"id":"RS7zkOEmjjPz"}},{"cell_type":"code","source":["!pip install torchtext==0.6.0"],"metadata":{"id":"cq84vuGrPmzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchtext\n","print(torchtext.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FYkNTw7rkYgY","executionInfo":{"status":"ok","timestamp":1711721862269,"user_tz":-540,"elapsed":539,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"b8f005ce-623c-4c9e-fa9a-e344abfc333b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6.0\n"]}]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"Ftz8g5Dgke_B"}},{"cell_type":"code","source":["!python -m spacy download en # 영어 전처리 모듈 설치\n","!python -m spacy download de # 독일어 전처리 모듈 설치"],"metadata":{"id":"snyV12hvlM0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### spaCy 라이브러리"],"metadata":{"id":"-CzJdSx0hGZz"}},{"cell_type":"code","source":["import spacy\n","\n","spacy_en=spacy.load('en_core_web_sm') # 영어 토큰화\n","spacy_de=spacy.load('de_core_news_sm') # 독일어 토큰화"],"metadata":{"id":"F-ZZ0xVZe2Nm","executionInfo":{"status":"ok","timestamp":1711721940818,"user_tz":-540,"elapsed":5909,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# 토큰화 기능 써보기\n","tokenized=spacy_en.tokenizer(\"I am a graduate student.\")\n","\n","for i,token in enumerate(tokenized):\n","    print(f'index {i}: {token.text}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1SFUNMnff-H","executionInfo":{"status":"ok","timestamp":1711721940818,"user_tz":-540,"elapsed":5,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"a0729447-0f71-4fc6-f9b0-adad8a227de0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["index 0: I\n","index 1: am\n","index 2: a\n","index 3: graduate\n","index 4: student\n","index 5: .\n"]}]},{"cell_type":"code","source":["# 토크나이저\n","from torchtext.data.utils import get_tokenizer\n","\n","tokenize_en = get_tokenizer('spacy', language='en_core_web_sm') # 영어문장 토큰화\n","tokenize_de = get_tokenizer('spacy', language='de_core_news_sm') # 독어문장 토큰화"],"metadata":{"id":"r4aTKqSNgKd-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711722393248,"user_tz":-540,"elapsed":1913,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"8832d5d3-ff90-4cda-d63b-ff779dc401b6"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n","  warnings.warn(Warnings.W111)\n"]}]},{"cell_type":"markdown","source":["### field 라이브러리\n","+ Source(SRC) : de\n","+ Target(TRG) : en"],"metadata":{"id":"6bAya_iAgykB"}},{"cell_type":"code","source":["from torchtext.data import Field, BucketIterator\n","\n","SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n","TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)"],"metadata":{"id":"GLDh_SIHhSk8","executionInfo":{"status":"ok","timestamp":1711722398172,"user_tz":-540,"elapsed":6,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# 영어-독일어 번역 데이터셋 불러오기\n","from torchtext.datasets import Multi30k\n","\n","train_dataset = Multi30k(path='./dataset/multi30k/',exts=('train.de','train.en'), fields=(SRC,TRG))\n","valid_dataset = Multi30k(path='./dataset/multi30k/',exts=('val.de','val.en'), fields=(SRC,TRG))\n","test_dataset = Multi30k(path='./dataset/multi30k/',exts=('test_2016_flickr.de','test_2016_flickr.en'), fields=(SRC,TRG))"],"metadata":{"id":"MuGcpI6bhkd_","executionInfo":{"status":"ok","timestamp":1711722405708,"user_tz":-540,"elapsed":5122,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\")\n","print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\")\n","print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKc4jNqqrPwU","executionInfo":{"status":"ok","timestamp":1711722408265,"user_tz":-540,"elapsed":558,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"71de2ca5-4877-4b6a-f495-90744ea6b402"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["학습 데이터셋(training dataset) 크기: 29000개\n","평가 데이터셋(validation dataset) 크기: 1014개\n","테스트 데이터셋(testing dataset) 크기: 1000개\n"]}]},{"cell_type":"code","source":["# 학습 데이터 중 하나를 선택해 출력\n","print(vars(train_dataset.examples[30])['src'])\n","print(vars(train_dataset.examples[30])['trg'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pqYFDTM3fF5","executionInfo":{"status":"ok","timestamp":1711722410637,"user_tz":-540,"elapsed":428,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"9cc9355c-7b0e-4ef4-f67c-37e6a109ab7f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["['ein', 'mann', ',', 'der', 'mit', 'einer', 'tasse', 'kaffee', 'an', 'einem', 'urinal', 'steht', '.']\n","['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"]}]},{"cell_type":"code","source":["print(*train_dataset.examples[30].src,sep=' ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I7AWKkWk6Pva","executionInfo":{"status":"ok","timestamp":1711722416003,"user_tz":-540,"elapsed":437,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"bfb7f1b7-0ed9-4dc4-8fe3-6332683b0a70"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["ein mann , der mit einer tasse kaffee an einem urinal steht .\n"]}]},{"cell_type":"code","source":["# build_vocab을 이용해 영어/독어 단어 사전 생성\n","## 최소 2번 이상 등장한 단어만 선택\n","\n","SRC.build_vocab(train_dataset,min_freq=2)\n","TRG.build_vocab(train_dataset,min_freq=2)\n","\n","print(f'len(SRC): {len(SRC.vocab)}')\n","print(f'len(TRG): {len(TRG.vocab)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7O0plmi-3k_A","executionInfo":{"status":"ok","timestamp":1711722420414,"user_tz":-540,"elapsed":1199,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"f88eda9c-c8a4-4a7a-c02b-395e03cd437f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["len(SRC): 7853\n","len(TRG): 5893\n"]}]},{"cell_type":"code","source":["## .stoi(string->int)를 통해서 단어 사전에서의\n","## 특정 단어와 맵핑된 고유한 정수를 출력\n","print(TRG.vocab.stoi['abcabc']) # <unk>=0 없는 단\n","print(TRG.vocab.stoi[TRG.pad_token]) # <pad>=1\n","print(TRG.vocab.stoi[\"<sos>\"]) # <sos>: 2\n","print(TRG.vocab.stoi[\"<eos>\"]) # <eos>: 3\n","print(TRG.vocab.stoi[\"hello\"])\n","print(TRG.vocab.stoi[\"world\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWxqMmJM4EBj","executionInfo":{"status":"ok","timestamp":1711723798613,"user_tz":-540,"elapsed":410,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"7dfcac20-f5cb-4a25-def9-92e0f1f5626f"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4112\n","1752\n"]}]},{"cell_type":"markdown","source":["### iterator\n","+ 한 문장의 단어들이 순서대로 네트워크에 입력되어야 함\n","+ 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들기\n","    + 이를 위해 BucketIterator를 사"],"metadata":{"id":"NRnxbuKVldLF"}},{"cell_type":"code","source":["# 한 문장의 단어들이 순서대로 네트워크에 입력되어야 함\n","## 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들기\n","## 이를 위해 BucketIterator를 사용합니다.\n","## 배치 크기(batch size): 128\n","import torch\n","\n","device=torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE=128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_dataset,valid_dataset,test_dataset),\n","    batch_size=BATCH_SIZE,\n","    device=device,\n","\n",")"],"metadata":{"id":"dQ0eNWAu5cOi","executionInfo":{"status":"ok","timestamp":1711722936230,"user_tz":-540,"elapsed":556,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["for i,batch in enumerate(train_iterator):\n","    src = batch.src.permute(1,0)\n","    trg = batch.trg.permute(1,0)\n","\n","    print(f'첫번째 배치 크기: {trg.shape}') # 단어개수 x 배치크기\n","\n","    # 현재 배치의 첫번째 문장 출력\n","    for i in range(trg.shape[0]):\n","        word_num = trg[i][0].item()\n","        print(f'index {i}: {word_num} => {TRG.vocab.itos[word_num]}')\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJPnEWiz7ysS","executionInfo":{"status":"ok","timestamp":1711723886521,"user_tz":-540,"elapsed":6,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"b6976747-0a31-4d1f-d50e-01031c2fefbe"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["첫번째 배치 크기: torch.Size([28, 128])\n","index 0: 2 => <sos>\n","index 1: 209 => this\n","index 2: 10 => is\n","index 3: 4 => a\n","index 4: 9 => man\n","index 5: 36 => standing\n","index 6: 8 => on\n","index 7: 4 => a\n","index 8: 2059 => rooftop\n","index 9: 13 => with\n","index 10: 216 => construction\n","index 11: 6 => in\n","index 12: 7 => the\n","index 13: 98 => background\n","index 14: 5 => .\n","index 15: 3 => <eos>\n","index 16: 1 => <pad>\n","index 17: 1 => <pad>\n","index 18: 1 => <pad>\n","index 19: 1 => <pad>\n","index 20: 1 => <pad>\n","index 21: 1 => <pad>\n","index 22: 1 => <pad>\n","index 23: 1 => <pad>\n","index 24: 1 => <pad>\n","index 25: 1 => <pad>\n","index 26: 1 => <pad>\n","index 27: 1 => <pad>\n"]}]},{"cell_type":"markdown","source":["## 인코더(Encoder) 아키텍쳐\n","+ SRC 문장을 Context Vector로 인코딩\n","+ LSTM은 hidden state와 cell state를 반환\n","    + **input_dim**: 단어들을 One-Hot 인코딩할 때의 하나의 단어의 차원\n","    + **embed_dim**: 임베딩 차원\n","    + **hidden_dim**\n","    + **n_layers**: RNN 레이어 개수\n","    + **dropout_ratio**"],"metadata":{"id":"WTdW24U0J-N3"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"_jIoH2EAKN98","executionInfo":{"status":"ok","timestamp":1711723908067,"user_tz":-540,"elapsed":10,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        # 단어 고유 정수값 -> embed_dim 차원으로 임베딩\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","        # 드롭아웃\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    # 소스문장(src)을 입력으로 받아 Context vector로 반환\n","    def forward(self, src):\n","        # src: [배치크기 x 단어개수], 각 단어의 고유 int 정보\n","        ## print(f'src.shape : {src.shape} -> ',end='')\n","        embedded = self.dropout(self.embedding(src))\n","        # embedded: [배치크기 x 단어개수 x 임베딩 차원]\n","        ## print(f'embedded.shape: {embedded.shape}')\n","\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        # outputs: [단어 개수 x 배치크기 x 히든차원], 현재 단어의 출력 정보\n","        # hidden: [레이어 개수 x 배치크기 x 히든차원], 현재까지의 모든 단어의 정보\n","        # cell: [레이어 개수 x 배치크기 x 히든차원], 현재까지의 모든 단어의 정보\n","\n","        # Context vector 반환\n","        return hidden,cell\n"],"metadata":{"id":"EOeYUW7cg7Bl","executionInfo":{"status":"ok","timestamp":1711723973921,"user_tz":-540,"elapsed":9,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = len(SRC.vocab) # 7853\n","ENCODER_EMBED_DIM = 256\n","HIDDEN_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT_RATIO = 0.5"],"metadata":{"id":"tOIAdllXnGzd","executionInfo":{"status":"ok","timestamp":1711723913346,"user_tz":-540,"elapsed":446,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["enc_test = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS,ENC_DROPOUT_RATIO)\n","enc_test.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVUlHW3ZnK5y","executionInfo":{"status":"ok","timestamp":1711723928032,"user_tz":-540,"elapsed":3,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"c3ccd614-55c6-4023-d0fe-3c607e48096a"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoder(\n","  (embedding): Embedding(7853, 256)\n","  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["for i,batch in enumerate(train_iterator):\n","    src = batch.src.permute(1,0)\n","\n","    hidden, cell = enc_test(src)\n","    print(f'hidden.shape: {hidden.shape}' )\n","    print(f'cell.shape: {cell.shape}' )\n","\n","    break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9oHSeT0CCt9","executionInfo":{"status":"ok","timestamp":1711723932425,"user_tz":-540,"elapsed":638,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"a2998e81-c4e7-4d8e-a4d0-d0caedc1383b"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["src.shape : torch.Size([31, 128]) -> embedded.shape: torch.Size([31, 128, 256])\n","hidden.shape: torch.Size([2, 128, 512])\n","cell.shape: torch.Size([2, 128, 512])\n"]}]},{"cell_type":"markdown","source":["## 디코더(Decoder) 아키텍처\n","+ Context vector를 타겟 문장으로 디코딩\n","+ LSTM은 hidden state와 cell state를 반환\n","    + **output_dim**: 단어에 대한 원핫 인코딩 차원\n","    + **embed_dim**: 임베딩 차원\n","    + **hidden_dim**: hidden_state 차원\n","    + **n_layers**: RNN 레이어의 개수\n","    + **dropout_ratio**: 드롭아웃 비율"],"metadata":{"id":"XcvaoOBdMrxh"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n","        super().__init__()\n","\n","        # 한 단어의 원핫인코딩 차원 -> 특정차원으로 임베딩\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","        # LSTM 레이어\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout_ratio)\n","\n","        # FC 레이어 -> 인코더와 다른 부분\n","        self.output_dim = output_dim\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","        # 드롭아웃\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","    def forward(self, input, hidden, cell):\n","        # input: [배치크기], 단어의 개수는 항상 1개\n","        # hidden: [레이어개수 x 배치크기 x 히든차원]\n","        # cell: [레이어개수 x 배치크기 x 히든차원]\n","        input=input.unsqueeze(0)\n","        # [배치크기] -> [1 x 배치크기]\n","\n","        embedded = self.dropout(self.embedding(input))\n","\n","        ouput, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        # outputs: [단어 개수=1 x 배치크기 x 히든차원], 현재 단어의 출력 정보\n","        # hidden: [레이어 개수 x 배치크기 x 히든차원], 현재까지의 모든 단어의 정보\n","        # cell: [레이어 개수 x 배치크기 x 히든차원], 현재까지의 모든 단어의 정보\n","\n","        prediction = self.fc_out(ouput.squeeze(0))\n","\n","        return prediction, hidden, cell"],"metadata":{"id":"nfItP7mkeaX2","executionInfo":{"status":"ok","timestamp":1711723984951,"user_tz":-540,"elapsed":408,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["OUTPUT_DIM = len(TRG.vocab) # 5893\n","DECODER_EMBED_DIM = 256\n","HIDDEN_DIM = 512\n","N_LAYERS = 2\n","DEC_DROPOUT_RATIO = 0.5"],"metadata":{"id":"z_vjGFvRALPb","executionInfo":{"status":"ok","timestamp":1711723992736,"user_tz":-540,"elapsed":423,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["dec_test = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS,DEC_DROPOUT_RATIO)\n","dec_test.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcZ_NtjEAPGa","executionInfo":{"status":"ok","timestamp":1711724734494,"user_tz":-540,"elapsed":4,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"9cb0056c-4533-4805-acdb-d5f9da5ab159"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Decoder(\n","  (embedding): Embedding(5893, 256)\n","  (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","  (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["print(outputs[:,0].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjqEJe-swf_t","executionInfo":{"status":"ok","timestamp":1711724965390,"user_tz":-540,"elapsed":5,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"0ccbb757-21f4-45dd-cc69-24c61901d642"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([30, 5893])\n"]}]},{"cell_type":"code","source":["import random\n","\n","for i,batch in enumerate(train_iterator):\n","    trg = batch.trg.permute(1,0)\n","\n","    print(trg.shape)\n","\n","    trg_len = trg.shape[0] # 단어 개수\n","    batch_size = trg.shape[1] # 배치 크기\n","    trg_vocab_size = dec_test.output_dim # 출력 차원\n","    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n","    # [단어개수 x 배치사이즈 x 출력차원] 공간만들기\n","\n","    input = trg[0] # [배치사이즈] 첫 입력은 항상 <sos>\n","\n","    print(f' 첫번째 문장\\n {outputs[:,0]}({outputs[:,0].shape})')\n","    for t in range(1, trg_len):\n","        output, hidden, cell = dec_test(input, hidden, cell)\n","\n","        outputs[t] = output\n","        print(f' {t}번째 단어 채운 후\\n {outputs[:,0]}({outputs[:,0].shape})')\n","\n","        top1 = output.argmax(1) # 가장 높은 확률인 단어의 인덱스 추출\n","        teacher_force = random.random() < 0.5\n","        input = trg[t] if teacher_force else top1 # 현재의 출력 결과를 다음 입력에서 넣기\n","\n","    break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hkBkj48kAYR6","executionInfo":{"status":"ok","timestamp":1711725219772,"user_tz":-540,"elapsed":444,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"b50f2493-af5c-4b01-fed0-297163193d71"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([26, 128])\n"," 첫번째 문장\n"," tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')(torch.Size([26, 5893]))\n"," 1번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 2번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 3번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 4번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 5번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 6번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 7번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 8번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 9번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 10번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 11번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 12번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 13번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 14번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 15번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 16번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 17번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 18번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 19번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 20번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 21번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 22번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 23번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0217,  0.0198,  0.0107,  ..., -0.0122, -0.0093, -0.0082],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 24번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0217,  0.0198,  0.0107,  ..., -0.0122, -0.0093, -0.0082],\n","        [ 0.0273, -0.0028, -0.0034,  ..., -0.0109,  0.0119, -0.0137],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"," 25번째 단어 채운 후\n"," tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0350,  0.0233,  0.0275,  ...,  0.0378, -0.0077, -0.0199],\n","        [ 0.0499, -0.0053,  0.0175,  ..., -0.0025,  0.0018, -0.0103],\n","        ...,\n","        [ 0.0217,  0.0198,  0.0107,  ..., -0.0122, -0.0093, -0.0082],\n","        [ 0.0273, -0.0028, -0.0034,  ..., -0.0109,  0.0119, -0.0137],\n","        [ 0.0409,  0.0035, -0.0032,  ...,  0.0030,  0.0074, -0.0242]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)(torch.Size([26, 5893]))\n"]}]},{"cell_type":"markdown","source":["## Seq2Seq 아키텍쳐\n","+ **인코더**: 주어진 소스문장을 context vector로 인코딩\n","+ **디코더**: 주어진context vector를 타겟 문장으로 디코딩\n","    + 한번에 하나의 단어씩 넣어서 구현"],"metadata":{"id":"Gzzazc5vR-fs"}},{"cell_type":"code","source":["import random\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        hidden, cell= self.encoder(src)\n","\n","        trg_len = trg.shape[0] # 단어개수\n","        batch_size = trg.shape[1] # 배치크기\n","        trg_vocab_size = self.decoder.output_dim # 출력 차원\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        input = trg[0, :]\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            outputs[t] = output\n","            top1 = output.argmax(1)\n","\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            input = trg[t] if teacher_force else top1\n","\n","        return outputs"],"metadata":{"id":"vbTzE4QKQcSZ","executionInfo":{"status":"ok","timestamp":1711725258533,"user_tz":-540,"elapsed":406,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"xrJOzALAVixg"}},{"cell_type":"code","source":["INPUT_DIM = len(SRC.vocab) # 7853\n","OUTPUT_DIM = len(TRG.vocab) # 5893\n","ENCODER_EMBED_DIM = 256\n","DECODER_EMBED_DIM = 256\n","HIDDEN_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT_RATIO = 0.5\n","DEC_DROPOUT_RATIO = 0.5"],"metadata":{"id":"4BYaHOqPSDbr","executionInfo":{"status":"ok","timestamp":1711725261699,"user_tz":-540,"elapsed":406,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["# 인코더(encoder)와 디코더(decoder) 객체 선언\n","enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n","dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n","\n","# Seq2Seq 객체 선언\n","model = Seq2Seq(enc, dec, device).to(device)"],"metadata":{"id":"_WKshQ-DSExr","executionInfo":{"status":"ok","timestamp":1711725263845,"user_tz":-540,"elapsed":2,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","\n","model.apply(init_weights) # 가중치초기화"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYxIaiLiSGUC","executionInfo":{"status":"ok","timestamp":1711725266382,"user_tz":-540,"elapsed":416,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"f788adf4-188f-4db9-d047-e0fbf80a02e4"},"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7853, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5893, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Adam optimizer로 학습 최적화\n","optimizer = optim.Adam(model.parameters())\n","\n","# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"],"metadata":{"id":"7FXJBPtJSP6G","executionInfo":{"status":"ok","timestamp":1711725275399,"user_tz":-540,"elapsed":2370,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["for i,batch in enumerate(train_iterator):\n","    src = batch.src.permute(1,0)\n","    trg = batch.trg.permute(1,0)\n","\n","    print(trg[1:])\n","    print(trg[1:].reshape(-1))\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GC05j5Ki1ZDj","executionInfo":{"status":"ok","timestamp":1711726409543,"user_tz":-540,"elapsed":5,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"8dedef5c-b17f-44bf-afe3-95dd2a668988"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[   4,   16,   16,  ...,    4,   16,   14],\n","        [   9, 1227,   59,  ...,  354,   24,    6],\n","        [  22,   17,  282,  ...,  583,  127,   25],\n","        ...,\n","        [   1,    1,    1,  ...,    1,    1,    1],\n","        [   1,    1,    1,  ...,    1,    1,    1],\n","        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\n","tensor([ 4, 16, 16,  ...,  1,  1,  1], device='cuda:0')\n"]}]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion, clip):\n","    model.train() # 학습모드\n","    epoch_loss = 0\n","\n","    for i, batch in enumerate(iterator):\n","        src = batch.src.permute(1,0)\n","        trg = batch.trg.permute(1,0)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg) # [단어개수 x 배치크기 x 출력차원]\n","        output_dim = output.shape[-1] # 출력차원 == trg vocab size\n","\n","        output = output[1:] # 시작단어는 사용하지 않음\n","        output = output.view(-1, output_dim) # [ (단어개수-1) * 배치크기 x 출력차원] -> 예측값의 확률들\n","        trg = trg[1:].reshape(-1) # [ (단어개수-1) * 배치크기 ] -> 정답값\n","\n","        loss = criterion(output, trg)\n","        loss.backward() # 기울기(gradient) 계산\n","\n","        # 기울기(gradient) clipping 진행\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        # 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 전체 손실 값 계산\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n"],"metadata":{"id":"u4-l5jVhyHqh","executionInfo":{"status":"ok","timestamp":1711726561316,"user_tz":-540,"elapsed":7,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["# 모델 평가(evaluate) 함수\n","def evaluate(model, iterator, criterion):\n","    model.eval() # 평가 모드\n","    epoch_loss = 0\n","\n","    with torch.no_grad():\n","        # 전체 평가 데이터를 확인하며\n","        for i, batch in enumerate(iterator):\n","            src = batch.src.permute(1,0)\n","            trg = batch.trg.permute(1,0)\n","\n","            # 평가할 때 teacher forcing는 사용하지 않음\n","            output = model(src, trg, 0)\n","            # output: [출력 단어 개수, 배치 크기, 출력 차원]\n","            output_dim = output.shape[-1]\n","\n","            # 출력 단어의 인덱스 0은 사용하지 않음\n","            output = output[1:].view(-1, output_dim)\n","            # output = [(출력 단어의 개수 - 1) * batch size, output dim]\n","            trg = trg[1:].reshape(-1)\n","            # trg = [(타겟 단어의 개수 - 1) * batch size]\n","\n","            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n","            loss = criterion(output, trg)\n","\n","            # 전체 손실 값 계산\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)"],"metadata":{"id":"10d8nnvbSZ0A","executionInfo":{"status":"ok","timestamp":1711726562892,"user_tz":-540,"elapsed":7,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"metadata":{"id":"KvzD2vya0PuE","executionInfo":{"status":"ok","timestamp":1711726564396,"user_tz":-540,"elapsed":3,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":["### 학습"],"metadata":{"id":"KHzi9d0x4czK"}},{"cell_type":"code","source":["import time\n","import math\n","import random\n","\n","N_EPOCHS = 20\n","CLIP = 1\n","best_valid_loss = float('inf')"],"metadata":{"id":"6HeIlZtI0R88","executionInfo":{"status":"ok","timestamp":1711726564871,"user_tz":-540,"elapsed":8,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["# 학습 시\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time() # 시작시간 기록\n","\n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time() # 종료시간 기록\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), './pt/seq2seq.pt')\n","\n","    print(f'Epoch: {epoch +1:02} | Time {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCQ0Gn2r0YVx","executionInfo":{"status":"ok","timestamp":1711727220736,"user_tz":-540,"elapsed":654455,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"68c729d0-1795-4e5b-c628-5b692d9acff0"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time 0m 32s\n","\tTrain Loss: 5.046 | Train PPL: 155.458\n","\t Val. Loss: 5.873 |  Val. PPL: 355.236\n","Epoch: 02 | Time 0m 31s\n","\tTrain Loss: 4.458 | Train PPL: 86.282\n","\t Val. Loss: 4.750 |  Val. PPL: 115.575\n","Epoch: 03 | Time 0m 32s\n","\tTrain Loss: 4.136 | Train PPL: 62.553\n","\t Val. Loss: 4.515 |  Val. PPL: 91.382\n","Epoch: 04 | Time 0m 32s\n","\tTrain Loss: 3.927 | Train PPL: 50.769\n","\t Val. Loss: 4.476 |  Val. PPL: 87.895\n","Epoch: 05 | Time 0m 33s\n","\tTrain Loss: 3.770 | Train PPL: 43.377\n","\t Val. Loss: 4.333 |  Val. PPL: 76.203\n","Epoch: 06 | Time 0m 31s\n","\tTrain Loss: 3.618 | Train PPL: 37.254\n","\t Val. Loss: 4.148 |  Val. PPL: 63.277\n","Epoch: 07 | Time 0m 32s\n","\tTrain Loss: 3.466 | Train PPL: 31.994\n","\t Val. Loss: 4.076 |  Val. PPL: 58.928\n","Epoch: 08 | Time 0m 31s\n","\tTrain Loss: 3.316 | Train PPL: 27.556\n","\t Val. Loss: 3.904 |  Val. PPL: 49.583\n","Epoch: 09 | Time 0m 32s\n","\tTrain Loss: 3.182 | Train PPL: 24.088\n","\t Val. Loss: 3.897 |  Val. PPL: 49.231\n","Epoch: 10 | Time 0m 31s\n","\tTrain Loss: 3.081 | Train PPL: 21.790\n","\t Val. Loss: 3.844 |  Val. PPL: 46.726\n","Epoch: 11 | Time 0m 33s\n","\tTrain Loss: 2.950 | Train PPL: 19.100\n","\t Val. Loss: 3.790 |  Val. PPL: 44.275\n","Epoch: 12 | Time 0m 31s\n","\tTrain Loss: 2.854 | Train PPL: 17.364\n","\t Val. Loss: 3.701 |  Val. PPL: 40.468\n","Epoch: 13 | Time 0m 32s\n","\tTrain Loss: 2.750 | Train PPL: 15.646\n","\t Val. Loss: 3.770 |  Val. PPL: 43.392\n","Epoch: 14 | Time 0m 32s\n","\tTrain Loss: 2.654 | Train PPL: 14.204\n","\t Val. Loss: 3.737 |  Val. PPL: 41.989\n","Epoch: 15 | Time 0m 35s\n","\tTrain Loss: 2.564 | Train PPL: 12.982\n","\t Val. Loss: 3.731 |  Val. PPL: 41.708\n","Epoch: 16 | Time 0m 33s\n","\tTrain Loss: 2.500 | Train PPL: 12.183\n","\t Val. Loss: 3.738 |  Val. PPL: 41.998\n","Epoch: 17 | Time 0m 34s\n","\tTrain Loss: 2.420 | Train PPL: 11.241\n","\t Val. Loss: 3.675 |  Val. PPL: 39.442\n","Epoch: 18 | Time 0m 32s\n","\tTrain Loss: 2.354 | Train PPL: 10.526\n","\t Val. Loss: 3.687 |  Val. PPL: 39.936\n","Epoch: 19 | Time 0m 32s\n","\tTrain Loss: 2.263 | Train PPL: 9.608\n","\t Val. Loss: 3.703 |  Val. PPL: 40.553\n","Epoch: 20 | Time 0m 32s\n","\tTrain Loss: 2.206 | Train PPL: 9.076\n","\t Val. Loss: 3.671 |  Val. PPL: 39.289\n"]}]},{"cell_type":"code","source":["# 모델 구조 저장\n","torch.save(model, './model/model.pth')"],"metadata":{"id":"TecC70XK77MO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습된 모델 로컬에 저장\n","from google.colab import files\n","\n","files.download('./pt/seq2seq.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"bcJnyddc1P2o","executionInfo":{"status":"ok","timestamp":1711727351969,"user_tz":-540,"elapsed":429,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"adb4f8e4-e779-4f03-b041-e00b382f622b"},"execution_count":118,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_10ffb03d-8862-44b8-b2be-4f6e1347377d\", \"seq2seq.pt\", 55597960)"]},"metadata":{}}]},{"cell_type":"markdown","source":["### 모델 결과 확인"],"metadata":{"id":"VMHCqU2c6GqN"}},{"cell_type":"code","source":["# 모델 구조 & 파라미터 가져오기\n","model2=torch.load('./model/model.pth')\n","model2.load_state_dict(torch.load('./pt/seq2seq.pt'))\n","model2.eval()\n","\n","# test score\n","test_loss = evaluate(model2, test_iterator, criterion)\n","print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hI_ZEp7d8A0H","executionInfo":{"status":"ok","timestamp":1711728144859,"user_tz":-540,"elapsed":1213,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"fae8ac23-40c7-441c-9cac-db22e7e3e05f"},"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 3.694 | Test PPL: 40.218\n"]}]},{"cell_type":"markdown","source":["### 번역해보기"],"metadata":{"id":"V0XQYAwu8Z8K"}},{"cell_type":"code","source":["def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n","    model.eval()\n","\n","    if isinstance(sentence, str):\n","        nlp = spacy.load('de')\n","        tokens = [token.text.lower() for token in nlp(sentence)]\n","    else:\n","        tokens =  [token.lower() for token in sentence]\n","\n","    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","    print(f\"전체 소스 토큰: {tokens}\")\n","\n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","    print(f\"소스 문장 인덱스: {src_indexes}\")\n","\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n","\n","    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n","    with torch.no_grad():\n","        hidden, cell = model.encoder(src_tensor)\n","\n","    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n","    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n","\n","    for i in range(max_len):\n","        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n","        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n","\n","        with torch.no_grad():\n","            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n","\n","        pred_token = output.argmax(1).item()\n","        trg_indexes.append(pred_token) # 출력 문장에 더하기\n","\n","        # <eos>를 만나는 순간 끝\n","        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n","            break\n","\n","    # 각 출력 단어 인덱스를 실제 단어로 변환\n","    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n","\n","    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n","    return trg_tokens[1:]"],"metadata":{"id":"SWbNOgys8ssK","executionInfo":{"status":"ok","timestamp":1711728466805,"user_tz":-540,"elapsed":516,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":138,"outputs":[]},{"cell_type":"code","source":["example_idx = 10\n","\n","src = vars(test_dataset.examples[example_idx])['src']\n","trg = vars(test_dataset.examples[example_idx])['trg']\n","\n","print(f'소스 문장: {src}')\n","print(f'타겟 문장: {trg}')\n","print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zlk4fZnU9o9K","executionInfo":{"status":"ok","timestamp":1711728467260,"user_tz":-540,"elapsed":3,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"da016a92-5996-4157-afae-85639ac51cbb"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["소스 문장: ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.']\n","타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n","전체 소스 토큰: ['<sos>', 'eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genießen', 'einen', 'schönen', 'tag', 'im', 'freien', '.', '<eos>']\n","소스 문장 인덱스: [2, 8, 364, 10, 134, 70, 624, 565, 19, 780, 200, 20, 88, 4, 3]\n","모델 출력 결과: a mother and her mother are enjoying a picture in a city . <eos>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"o_imn3wj9qw8"},"execution_count":null,"outputs":[]}]}