{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["43fjWFsWc8fa","3RHsD5-NdUuM"],"toc_visible":true,"mount_file_id":"1wUi5NIjRTGqszqGBzOeh2bRm_LX9pzFd","authorship_tag":"ABX9TyOKFxUHpIv+nGEQik7asTMX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["해당 노트북은 [Link](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb)를 참고하여 제작되었습니다."],"metadata":{"id":"G7Hlmx_Tcjsa"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOufTjAC2MQp","executionInfo":{"status":"ok","timestamp":1706953535165,"user_tz":-540,"elapsed":18109,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"3d9fe773-7f51-449d-ad1a-10c91e22184e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/AIKU/Github/AIKU-DL-Paper-Review/code_practices/Transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3IRaDIi227i","executionInfo":{"status":"ok","timestamp":1706953599113,"user_tz":-540,"elapsed":586,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"2406bd2b-f062-4442-84a5-7333674224b3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AIKU/Github/AIKU-DL-Paper-Review/code_practices/Transformer\n"]}]},{"cell_type":"markdown","source":["## BLEU Score 계산을 위한 라이브러리"],"metadata":{"id":"43fjWFsWc8fa"}},{"cell_type":"code","source":["!pip install torchtext==0.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCZSyFRodLnZ","executionInfo":{"status":"ok","timestamp":1706693491476,"user_tz":-540,"elapsed":6454,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"477cc0f4-aa10-4757-8681-897572e60b87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n","Installing collected packages: torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.16.0\n","    Uninstalling torchtext-0.16.0:\n","      Successfully uninstalled torchtext-0.16.0\n","Successfully installed torchtext-0.6.0\n"]}]},{"cell_type":"code","source":["import torchtext\n","print(torchtext.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WhYa7bg_ttw1","executionInfo":{"status":"ok","timestamp":1706693503113,"user_tz":-540,"elapsed":5769,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"67492940-70ea-4138-d4a9-15cb696a1718"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6.0\n"]}]},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"3RHsD5-NdUuM"}},{"cell_type":"code","source":["%%capture\n","!python -m spacy download en # 영어 전처리 모듈 설치\n","!python -m spacy download de # 독일어 전처리 모듈 설치"],"metadata":{"id":"9DDnYGr6de_j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### spaCy 라이브러리"],"metadata":{"id":"-CzJdSx0hGZz"}},{"cell_type":"code","source":["import spacy\n","\n","spacy_en=spacy.load('en_core_web_sm') # 영어 토큰화\n","spacy_de=spacy.load('de_core_news_sm') # 독일어 토큰화"],"metadata":{"id":"F-ZZ0xVZe2Nm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토큰화 기능 써보기\n","tokenized=spacy_en.tokenizer(\"I am a graduate student.\")\n","\n","for i,token in enumerate(tokenized):\n","    print(f'index {i}: {token.text}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1SFUNMnff-H","executionInfo":{"status":"ok","timestamp":1706693567411,"user_tz":-540,"elapsed":8,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"543b2bb2-6654-4989-e51e-3c94c07c8682"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["index 0: I\n","index 1: am\n","index 2: a\n","index 3: graduate\n","index 4: student\n","index 5: .\n"]}]},{"cell_type":"code","source":["# 영어 문장을 토큰화하는 함수\n","def tokenize_en(text):\n","    return [token.text for token in spacy_en.tokenizer(text)]\n","# 독일어 문장을 토큰화하는 함수\n","def tokenize_de(text):\n","    return [token.text for token in spacy_de.tokenizer(text)]"],"metadata":{"id":"r4aTKqSNgKd-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### field 라이브러리\n","+ Source(SRC) : de\n","+ Target(TRG) : en"],"metadata":{"id":"6bAya_iAgykB"}},{"cell_type":"code","source":["from torchtext.data import Field, BucketIterator\n","\n","SRC = Field(tokenize=tokenize_de, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)\n","TRG = Field(tokenize=tokenize_en, init_token=\"<sos>\", eos_token=\"<eos>\", lower=True, batch_first=True)"],"metadata":{"id":"GLDh_SIHhSk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 영어-독일어 번역 데이터셋 불러오기\n","from torchtext.datasets import Multi30k\n","\n","train_dataset = Multi30k(path='./dataset/multi30k/',exts=('train.de','train.en'), fields=(SRC,TRG))\n","valid_dataset = Multi30k(path='./dataset/multi30k/',exts=('val.de','val.en'), fields=(SRC,TRG))\n","test_dataset = Multi30k(path='./dataset/multi30k/',exts=('test_2016_flickr.de','test_2016_flickr.en'), fields=(SRC,TRG))"],"metadata":{"id":"MuGcpI6bhkd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"학습 데이터셋(training dataset) 크기: {len(train_dataset.examples)}개\")\n","print(f\"평가 데이터셋(validation dataset) 크기: {len(valid_dataset.examples)}개\")\n","print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_dataset.examples)}개\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKc4jNqqrPwU","executionInfo":{"status":"ok","timestamp":1706693621318,"user_tz":-540,"elapsed":10,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"bf3f3ab5-fbc9-49d1-934e-3fe62edb01bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["학습 데이터셋(training dataset) 크기: 29000개\n","평가 데이터셋(validation dataset) 크기: 1014개\n","테스트 데이터셋(testing dataset) 크기: 1000개\n"]}]},{"cell_type":"code","source":["# 학습 데이터 중 하나를 선택해 출력\n","print(vars(train_dataset.examples[30])['src'])\n","print(vars(train_dataset.examples[30])['trg'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pqYFDTM3fF5","executionInfo":{"status":"ok","timestamp":1706693645430,"user_tz":-540,"elapsed":333,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"95b5b722-23a8-4ddf-8f70-74841b500a91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['ein', 'mann', ',', 'der', 'mit', 'einer', 'tasse', 'kaffee', 'an', 'einem', 'urinal', 'steht', '.']\n","['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"]}]},{"cell_type":"code","source":["print(*train_dataset.examples[30].src,sep=' ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I7AWKkWk6Pva","executionInfo":{"status":"ok","timestamp":1706694449877,"user_tz":-540,"elapsed":15,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"f6fc114f-042a-4f34-ea9e-d0ebc1e9e9e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ein mann , der mit einer tasse kaffee an einem urinal steht .\n"]}]},{"cell_type":"code","source":["# build_vocab을 이용해 영어/독어 단어 사전 생성\n","## 최소 2번 이상 등장한 단어만 선택\n","\n","SRC.build_vocab(train_dataset,min_freq=2)\n","TRG.build_vocab(train_dataset,min_freq=2)\n","\n","print(f'len(SRC): {len(SRC.vocab)}')\n","print(f'len(TRG): {len(TRG.vocab)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7O0plmi-3k_A","executionInfo":{"status":"ok","timestamp":1706693773220,"user_tz":-540,"elapsed":330,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"76f58872-55bc-4963-f6fb-6f46f2f2e005"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(SRC): 7853\n","len(TRG): 5893\n"]}]},{"cell_type":"code","source":["## .stoi(string->int)를 통해서 단어 사전에서의\n","## 특정 단어와 맵핑된 고유한 정수를 출력\n","print(TRG.vocab.stoi['abcabc']) # <unk>=0 없는 단\n","print(TRG.vocab.stoi[TRG.pad_token]) # <pad>=1\n","print(TRG.vocab.stoi[\"<sos>\"]) # <sos>: 2\n","print(TRG.vocab.stoi[\"<eos>\"]) # <eos>: 3\n","print(TRG.vocab.stoi[\"hello\"])\n","print(TRG.vocab.stoi[\"world\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BWxqMmJM4EBj","executionInfo":{"status":"ok","timestamp":1706694256443,"user_tz":-540,"elapsed":497,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"aeae3fae-97bd-428d-b39f-8aac91f0f23f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4112\n","1752\n"]}]},{"cell_type":"code","source":["# 한 문장의 단어들이 순서대로 네트워크에 입력되어야 함\n","## 따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들기\n","## 이를 위해 BucketIterator를 사용합니다.\n","## 배치 크기(batch size): 128\n","import torch\n","\n","device=torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE=128\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_dataset,valid_dataset,test_dataset),\n","    batch_size=BATCH_SIZE,\n","    device=device\n",")"],"metadata":{"id":"dQ0eNWAu5cOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i,batch in enumerate(train_iterator):\n","    src=batch.src\n","    trg=batch.trg\n","\n","    print(f'첫번째 배치 크기: {trg.shape}')\n","\n","    # 현재 배치의 첫번째 문장 출력\n","    for idx,word in enumerate(trg[0]):\n","        print(f'idx{idx}: {word} => {TRG.vocab.itos[word]}')\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJPnEWiz7ysS","executionInfo":{"status":"ok","timestamp":1706696029631,"user_tz":-540,"elapsed":19,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"7d982726-9ef0-49fd-c385-64c5b5569147"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["첫번째 배치 크기: torch.Size([128, 26])\n","idx0: 2 => <sos>\n","idx1: 4 => a\n","idx2: 9 => man\n","idx3: 395 => selling\n","idx4: 893 => produce\n","idx5: 8 => on\n","idx6: 7 => the\n","idx7: 39 => street\n","idx8: 5 => .\n","idx9: 3 => <eos>\n","idx10: 1 => <pad>\n","idx11: 1 => <pad>\n","idx12: 1 => <pad>\n","idx13: 1 => <pad>\n","idx14: 1 => <pad>\n","idx15: 1 => <pad>\n","idx16: 1 => <pad>\n","idx17: 1 => <pad>\n","idx18: 1 => <pad>\n","idx19: 1 => <pad>\n","idx20: 1 => <pad>\n","idx21: 1 => <pad>\n","idx22: 1 => <pad>\n","idx23: 1 => <pad>\n","idx24: 1 => <pad>\n","idx25: 1 => <pad>\n"]}]},{"cell_type":"code","source":["train_iterator.batches"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GN0UOEgF8dwU","executionInfo":{"status":"ok","timestamp":1706695339283,"user_tz":-540,"elapsed":8,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"260b0259-8de0-4cc4-af72-b139be209a28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<generator object pool at 0x7aad4dd7cba0>"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["## Multi Head Attention 아키텍쳐"],"metadata":{"id":"n2pdS1ax9szI"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self,hidden_dim,n_heads,dropout_ratio,device):\n","        super().__init__()\n","\n","        assert hidden_dim%n_heads==0\n","\n","        self.hidden_dim=hidden_dim #임베딩 차원\n","        self.n_heads=n_heads #헤드 개수\n","        self.head_dim=hidden_dim//n_heads #각 헤드에서의 임베딩 차원\n","\n","        self.fc_q=nn.Linear(hidden_dim, hidden_dim) #W_Q\n","        self.fc_k=nn.Linear(hidden_dim, hidden_dim) #W_K\n","        self.fc_v=nn.Linear(hidden_dim, hidden_dim) #W_V\n","\n","        self.fc_o=nn.Linear(hidden_dim,hidden_dim) #W_O\n","\n","        self.dropout=nn.Dropout(dropout_ratio)\n","\n","        self.scale=torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) #srqt(d_k)\n","\n","    def forward(self,query,key,value,mask=None):\n","        batch_size=query.shape[0]\n","\n","        Q = self.fc_q(query) #XW_Q=Q [n_seq, hidden_dim] x [hidden_dim, hidden_dim] -> [n_seq, hidden_dim]\n","        K = self.fc_k(key) #XW_Q=Q [n_seq, hidden_dim] x [hidden_dim, hidden_dim] -> [n_seq, hidden_dim]\n","        V = self.fc_v(value) #XW_Q=Q [n_seq, hidden_dim] x [hidden_dim, hidden_dim] -> [n_seq, hidden_dim]\n","\n","        # hidden_dim → n_heads X head_dim 형태로 변형\n","        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n","        # [batch_size, seq_len, hidden_dim] -> [batch_size, seq_len, n_heads, head_dim] -> [batch_size, n_heads, seq_len, head_dim]\n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        # Attention Energy 계산\n","        # [seq_len x head_dim] * [head_dim x seq_len] -> [seq_len x seq_len]\n","        # energy: [batch_size, n_heads, query_len, key_len]\n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","\n","        # 마스크(mask)를 사용하는 경우\n","        if mask is not None:\n","            energy=energy.masked_fill(mask==0,-1e10)\n","            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n","\n","        # Attention Score 계산\n","        # [batch_size, n_heads, seq_len, seq_len]\n","        # attention: [batch_size, n_heads, query_len, key_len]\n","        attention = torch.softmax(energy, dim=-1) ## dim=-1: 다른 key끼리 더했을 때 1이 되도록\n","\n","        # Scaled Dot-Product Attention을 계산\n","        # [seq_len x seq_len] * [seq_len x head_dim] -> [seq_len x head_dim]\n","        # x: [batch_size, n_heads, query_len, head_dim]\n","        x = torch.matmul(self.dropout(attention), V)\n","        x = x.permute(0, 2, 1, 3).contiguous() ## 강제로 메모리 재할당하여 연속적으로 만들기\n","\n","        # [batch_size, n_heads, seq_len, head_dim] -> [batch_size, seq_len, hidden_dim]\n","        # hidden_dim=n_heads*head_dim\n","        x = x.view(batch_size, -1, self.hidden_dim)\n","\n","        # [batch_size, seq_len, hidden_dim]\n","        x = self.fc_o(x)\n","\n","        return x, attention"],"metadata":{"id":"oXenB372A1QR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Position-wise Feed-forward 아키텍처"],"metadata":{"id":"jxsmrGwNDuPG"}},{"cell_type":"code","source":["class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self,hidden_dim,pf_dim,dropout_ratio):\n","        super().__init__()\n","\n","        self.fc_1=nn.Linear(hidden_dim,pf_dim)\n","        self.fc_2=nn.Linear(pf_dim,hidden_dim)\n","\n","        self.dropout=nn.Dropout(dropout_ratio)\n","\n","    def forward(self,x):\n","        x=self.dropout(torch.relu(self.fc_1(x)))\n","        x=self.fc_2(x)\n","        return x"],"metadata":{"id":"QAiRm-C4T7C0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EncoderLayer"],"metadata":{"id":"sVcujnXZEAeu"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    def __init__(self,hidden_dim, n_heads, pf_dim,dropout_ratio,device):\n","        super().__init__()\n","\n","        self.self_attn_layer_norm=nn.LayerNorm(hidden_dim)\n","        self.ff_layer_norm=nn.LayerNorm(hidden_dim)\n","        self.self_attention=MultiHeadAttentionLayer(hidden_dim,n_heads,dropout_ratio,device)\n","        self.positionwise_feedforward=PositionwiseFeedforwardLayer(hidden_dim,pf_dim,dropout_ratio)\n","        self.dropout=nn.Dropout(dropout_ratio)\n","\n","    def forward(self,src,src_mask):\n","        # self-attention\n","        _src,_=self.self_attention(src,src,src,src_mask) ## query,key,value,mask=None\n","\n","        # dropout+residual -> Layernorm\n","        src=self.self_attn_layer_norm(src+self.dropout(_src))\n","\n","        # position-wise feedforward\n","        _src=self.positionwise_feedforward(src)\n","\n","        # dropout+residual -> Layernorm\n","        src=self.ff_layer_norm(src+self.dropout(_src))\n","\n","        return src"],"metadata":{"id":"cwKUW7xVVFTj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"g16WzfizEDci"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self,input_dim,hidden_dim,n_layers,n_heads,pf_dim,dropout_ratio,device,max_length=100):\n","        super().__init__()\n","\n","        self.device=device\n","\n","        self.tok_embedding=nn.Embedding(input_dim,hidden_dim)\n","        self.pos_embedding=nn.Embedding(max_length,hidden_dim) #position embedding\n","\n","        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n","\n","        self.dropout = nn.Dropout(dropout_ratio)\n","\n","        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n","\n","    def forward(self,src,src_mask):\n","        batch_size=src.shape[0]\n","        src_len=src.shape[1] #seq_len\n","\n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","\n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","\n","        return src\n"],"metadata":{"id":"nI05JcyiXaHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(sle,f)"],"metadata":{"id":"bOzqmE-Cq2wQ"},"execution_count":null,"outputs":[]}]}