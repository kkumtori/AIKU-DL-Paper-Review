{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPvZ1B38TipW4VAQQqkSNQ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["해당 노트북은 고려대학교 정보대학 딥러닝 학회 [AIKU](https://github.com/AIKU-Official) D2D 과제 내용을 바탕으로 작성되었습니다."],"metadata":{"id":"RSBBnxdPNywP"}},{"cell_type":"markdown","source":["# 환경설정"],"metadata":{"id":"At5a4OBaP_fx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","FOLDERNAME = '/AIKU/Github/AIKU-DL-Paper-Review/code_practices/PyTorch'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/datasets/\n","!bash get_datasets.sh\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KK_kQqu5PKza","executionInfo":{"status":"ok","timestamp":1706954656532,"user_tz":-540,"elapsed":4221,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"446d0276-ac00-45ac-c45b-984c5240d16f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/AIKU/Github/AIKU-DL-Paper-Review/code_practices/PyTorch/datasets\n","/content/drive/My Drive/AIKU/Github/AIKU-DL-Paper-Review/code_practices/PyTorch\n"]}]},{"cell_type":"code","source":["# GPU\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","\n","import torchvision.datasets as dset\n","import torchvision.transforms as T\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","USE_GPU=True\n","dtype=torch.float32\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device=torch.device('cuda')\n","else:\n","    device=torch.device('cpu')\n","\n","print('using device: ',device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6FKDvE1RcGe","executionInfo":{"status":"ok","timestamp":1706954660523,"user_tz":-540,"elapsed":709,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"2ee9b77b-d4c6-4a62-f5b8-1626679c0048"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["using device:  cuda\n"]}]},{"cell_type":"markdown","source":["# 목차\n","\n","+ **Part1**.   준비\n","+ **Part2**.   Barebones PyTorch : 추상화 수준 1\n","+ **Part3**.   PyTorch Module API : 추상화 수준 2/ `nn.Module` 사용\n","+ **Part4**.   PyTorch Sequential API :추상화 수준 3/ `nn.Sequential` 사용\n","+ **Part5**.   CIFAR-10 open-ended challenge\n","+ **Part6**.   Pretrained model\n","\n","<br>\n","  \n","참고: PyTorch 추상화 수준별 비교\n","    \n","| API | 유연성 | 편의성 |\n","|---------------|-------------|-------------|\n","| Barebones | 높음 | 낮음 |\n","| `nn.Module` | 높음 | 중간 |\n","| `nn.Sequential` | 낮음 | 높음 |\n"],"metadata":{"id":"4jQgpb1iPzeK"}},{"cell_type":"markdown","source":["# **Part1. 준비**"],"metadata":{"id":"dYeTRUG0SOMw"}},{"cell_type":"markdown","source":[" The `torchvision.transforms` package provides tools for **preprocessing data and for performing data augmentation**;  \n"," here we set up a transform to preprocess the data *by subtracting the mean RGB value and dividing by the standard deviation of each RGB value*;    \n"," we've hardcoded the mean and std."],"metadata":{"id":"8NM3KBRNXDKI"}},{"cell_type":"markdown","source":["## Preprocessing"],"metadata":{"id":"6kUoITFofZb5"}},{"cell_type":"code","source":["# Preprocessing\n","transform=T.Compose([\n","    T.ToTensor(),\n","    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","])"],"metadata":{"id":"hVrc1B6TSa76","executionInfo":{"status":"ok","timestamp":1706954664822,"user_tz":-540,"elapsed":11,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["We set up a Dataset object for each split (train / val / test);  \n","Datasets load training examples one at a time, so we wrap each Dataset in a `DataLoader` which iterates through the Dataset and forms minibatches.  \n","We divide the CIFAR-10 training set into train and val sets by passing a `Sampler` object to the DataLoader telling how it should sample from the underlying Dataset."],"metadata":{"id":"-H4eZ4jJXlfU"}},{"cell_type":"markdown","source":["## DataLoader"],"metadata":{"id":"ItwF9BlXfcQ0"}},{"cell_type":"code","source":["# DataLoader\n","NUM_TRAIN=49000\n","\n","cifar_10_train=dset.CIFAR10('./datasets',train=True,download=True,transform=transform)\n","loader_train=DataLoader(cifar_10_train,batch_size=64,\n","                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN))) #train 49,000장\n","\n","cifar_10_val=dset.CIFAR10('./datasets',train=True,download=True,transform=transform)\n","loader_val=DataLoader(cifar_10_train,batch_size=64,\n","                      sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN,50000))) #val1,000장\n","\n","cifar_10_test=dset.CIFAR10('./datasets',train=False,download=True,transform=transform)\n","loader_test=DataLoader(cifar_10_test,batch_size=64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GYx38G-TBaB","executionInfo":{"status":"ok","timestamp":1706959824323,"user_tz":-540,"elapsed":4934,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"c860e75c-2b0c-47ee-e65a-a8877d6a4292"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["## Original Images"],"metadata":{"id":"6gIERQBxfeRS"}},{"cell_type":"code","source":["def org_imshow(transformed_tensor):\n","    means=np.array([0.4914, 0.4822, 0.4465])\n","    stds=np.array([0.2023, 0.1994, 0.2010])\n","    tmp=[]\n","    for i in range(3):\n","        tmp.append(np.array((transformed_tensor[i]))*stds[i]+means[i])\n","    plt.imshow(np.transpose(tmp,(1,2,0)))"],"metadata":{"id":"rczf2TsOYPmA","executionInfo":{"status":"ok","timestamp":1706954685097,"user_tz":-540,"elapsed":18,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["for batch in loader_train:\n","    print('첫번째 배치 shape:',batch[0].shape)\n","    print('첫번째 배치 class:',batch[1].shape)\n","    org_imshow(batch[0][0])\n","    plt.title(cifar_10_train.classes[batch[1][0]])\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"DASQOguBVZPB","executionInfo":{"status":"ok","timestamp":1706954685841,"user_tz":-540,"elapsed":760,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"3086e39a-1d2a-48ba-a9b8-39aa72ee48fe"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["첫번째 배치 shape: torch.Size([64, 3, 32, 32])\n","첫번째 배치 class: torch.Size([64])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# **Part2. Barebones PyTorch**"],"metadata":{"id":"2SjHnxwVddT7"}},{"cell_type":"markdown","source":["## PyTorch Tensors: Flatten 함수"],"metadata":{"id":"Kcq4da63e0AJ"}},{"cell_type":"code","source":["def flatten(x):\n","    N=x.shape[0] # read in N,C,H,W\n","    return x.view(N,-1) # \"flatten\" the C * H * W values into a single vector per image"],"metadata":{"id":"f2b5sroNfot_","executionInfo":{"status":"ok","timestamp":1706954692152,"user_tz":-540,"elapsed":5,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def test_flatten():\n","    x=torch.arange(12).view(2,1,3,2)\n","    print('**Before flattening** \\n',x)\n","    print(x.shape)\n","    print('='*30+'>>')\n","    print('**After flattening** \\n',flatten(x))\n","    print(flatten(x).shape)"],"metadata":{"id":"FtPtPwQPgFUO","executionInfo":{"status":"ok","timestamp":1706957996402,"user_tz":-540,"elapsed":728,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["test_flatten()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08HwZnrrgf43","executionInfo":{"status":"ok","timestamp":1706957996403,"user_tz":-540,"elapsed":11,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"74e94ced-69f6-4f24-f2b4-c1b87313b208"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["**Before flattening** \n"," tensor([[[[ 0,  1],\n","          [ 2,  3],\n","          [ 4,  5]]],\n","\n","\n","        [[[ 6,  7],\n","          [ 8,  9],\n","          [10, 11]]]])\n","torch.Size([2, 1, 3, 2])\n","==============================>>\n","**After flattening** \n"," tensor([[ 0,  1,  2,  3,  4,  5],\n","        [ 6,  7,  8,  9, 10, 11]])\n","torch.Size([2, 6])\n"]}]},{"cell_type":"markdown","source":["## 2-1.Barebones PyTorch: Two-Layer Network\n","여기에서는 이미지 데이터 배치에 대해 완전히 연결된 2계층 ReLU 네트워크의 포워드 패스를 수행하는 함수 `two_layer_fc`를 정의합니다.  \n","포워드 패스를 정의한 후에는 네트워크를 통해 0을 실행하여 충돌이 발생하지 않는지, 올바른 모양의 출력을 생성하는지 확인합니다.  \n","여기서 코드를 작성할 필요는 없지만 구현을 읽고 이해하는 것이 중요합니다."],"metadata":{"id":"HUD_-vnWgg62"}},{"cell_type":"code","source":["import torch.nn.functional as F # useful stateless functions\n","\n","def two_layer_fc(x,params):\n","    \"\"\"\n","    A fully-connected neural networks; the architecture is:\n","    NN is fully connected -> ReLU -> fully connected layer.\n","    Note that this function only defines the forward pass;\n","    PyTorch will take care of the backward pass for us.\n","\n","    The input to the network will be a minibatch of data, of shape\n","    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n","    and the output layer will produce scores for C classes.\n","\n","    Inputs:\n","    - x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of\n","      input data.\n","    - params: A list [w1, w2] of PyTorch Tensors giving weights for the network;\n","      w1 has shape (D, H) and w2 has shape (H, C).\n","\n","    Returns:\n","    - scores: A PyTorch Tensor of shape (N, C) giving classification scores for\n","      the input data x.\n","    \"\"\"\n","\n","    # first we flatten the image\n","    x=flatten(x) # shape: [batch_size, C x H x W]->없어도 됨\n","\n","    w1,w2=params\n","\n","    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n","    # w2 have requires_grad=True, operations involving these Tensors will cause\n","    # PyTorch to build a computational graph, allowing automatic computation of\n","    # gradients. Since we are no longer implementing the backward pass by hand we\n","    # don't need to keep references to intermediate values.\n","    # you can also use `.clamp(min=0)`, equivalent to F.relu()\n","    x=F.relu(x.mm(w1)) #mm: matrix multiplication\n","    x=x.mm(w2)\n","    return x"],"metadata":{"id":"EwoDpGTBhSBR","executionInfo":{"status":"ok","timestamp":1706954704267,"user_tz":-540,"elapsed":9,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def two_layer_fc_test():\n","    hidden_layer_size=42\n","    x=torch.zeros((64,50),dtype=dtype)\n","    w1=torch.zeros((50,hidden_layer_size),dtype=dtype)\n","    w2=torch.zeros((hidden_layer_size,10),dtype=dtype)\n","    scores=two_layer_fc(x,[w1,w2])\n","    print(scores.size())\n","\n","two_layer_fc_test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z4GCNHVlisYQ","executionInfo":{"status":"ok","timestamp":1706954704271,"user_tz":-540,"elapsed":11,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"484e5606-0bb1-46c0-b018-77ff40bed961"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 10])\n"]}]},{"cell_type":"markdown","source":["## 2-2.Barebones PyTorch: Three-Layer ConvNet\n","\n","여기서는 3계층 Convolution 네트워크의 순방향 패스를 수행하는 `three_layer_convnet` 함수의 구현을 완료합니다. 위와 같이 네트워크에 0을 전달하여 구현을 즉시 테스트할 수 있습니다. 네트워크는 다음과 같은 구조를 가져야 합니다:\n","\n","1. `channel_1` 필터가 있는 Convolution 레이어(bias 포함), 각각 모양이 `KW1 x KH1`이고 zero padding 이 2입니다.\n","2. ReLU nonlinearity\n","3. `channel_2` 필터가 있는 Convolution 레이어(bias 포함), 각 필터의 모양이 `KW2 x KH2`이고 제로 패딩이 1입니다.\n","4. ReLU nonlinearity\n","5. bias가 있는 Fully-connected layer, C 클래스에 대한 점수를 생성합니다.\n","\n","Fully-connected layer 이후에는 **소프트맥스 활성화가 없음**에 유의하십시오: 이는 PyTorch의 교차 엔트로피 손실이 소프트맥스 활성화를 수행하기 때문이며, 이 단계를 번들로 묶으면 계산이 더 효율적이기 때문입니다."],"metadata":{"id":"oA_EbgVDj3jk"}},{"cell_type":"code","source":["def three_layer_convnet(x,params):\n","    \"\"\"\n","    Performs the forward pass of a three-layer convolutional network with the\n","    architecture defined above.\n","\n","    Inputs:\n","    - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images\n","    - params: A list of PyTorch Tensors giving the weights and biases for the\n","      network; should contain the following:\n","      - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights\n","        for the first convolutional layer\n","      - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first\n","        convolutional layer\n","      - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving\n","        weights for the second convolutional layer\n","      - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second\n","        convolutional layer\n","      - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you\n","        figure out what the shape should be?\n","      - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you\n","        figure out what the shape should be?\n","\n","    Returns:\n","    - scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n","    \"\"\"\n","    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n","    scores = None\n","\n","    x=F.conv2d(input=x,weight=conv_w1,bias=conv_b1,padding=2) #1\n","    x=F.relu(x) #2\n","    x=F.conv2d(input=x,weight=conv_w2,bias=conv_b2,padding=1) #3\n","    x=F.relu(x) #4\n","    x=flatten(x)\n","    scores=x.mm(fc_w)+fc_b #5\n","\n","    return scores"],"metadata":{"id":"bag6R6rRkVWw","executionInfo":{"status":"ok","timestamp":1706954708242,"user_tz":-540,"elapsed":736,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def three_layer_convnet_test():\n","    x=torch.zeros((64,3,32,32),dtype=dtype)\n","\n","    conv_w1=torch.zeros((6,3,5,5),dtype=dtype) # [out_channel, in_channel, kernel_H, kernel_W]\n","    conv_b1=torch.zeros((6,)) # out_channel\n","    conv_w2 = torch.zeros((9, 6, 3, 3), dtype=dtype)  # [out_channel, in_channel, kernel_H, kernel_W]\n","    conv_b2 = torch.zeros((9,))  # out_channel\n","\n","    fc_w = torch.zeros((9 * 32 * 32, 10))\n","    fc_b = torch.zeros(10)\n","\n","    scores = three_layer_convnet(x, [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b])\n","    print(scores.size())  # you should see [64, 10]\n","\n","three_layer_convnet_test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shJMAgqEl2HH","executionInfo":{"status":"ok","timestamp":1706954708864,"user_tz":-540,"elapsed":8,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"0c542215-3b6d-4efd-fdd1-a6fef42935b6"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 10])\n"]}]},{"cell_type":"markdown","source":["## 2-3.Barebones PyTorch: Initialization\n","모델의 가중치 행렬을 초기화하는 몇 가지 유틸리티 메서드를 작성해 보겠습니다.\n","\n","- `random_weight(shape)`는 Kaiming normalization 방법으로 가중치 텐서를 초기화합니다.\n","- `zero_weight(shape)`는 모든 0으로 가중치 텐서를 초기화합니다. 바이어스 매개변수를 인스턴스화할 때 유용합니다.\n","\n","`random_weight` 함수는 Kaiming normal initialization 방법을 사용합니다:\n","\n","+ He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*, ICCV 2015, https://arxiv.org/abs/1502.01852\n","\n","- [Kaiming He Normal Initialization](https://imlim0813.tistory.com/25)   \n","$X\\sim N(0,\\sqrt{\\frac{2}{fan_{in}}}^2)$\n","\n"],"metadata":{"id":"46FRIyZkmf26"}},{"cell_type":"code","source":["# Kaiming He Normal Initialization\n","def random_weight(shape):\n","    \"\"\"\n","    Create random Tensors for weights; setting requires_grad=True means that we\n","    want to compute gradients for these Tensors during the backward pass.\n","    We use Kaiming normalization: sqrt(2 / fan_in)\n","    \"\"\"\n","    if len(shape)==2: # FC layer [in_nodes,out_nodes]\n","        fan_in=shape[0]\n","    else:\n","        fan_in=np.prod(shape[1:]) # Conv layer [out_channel,in_channel,kH, kW]\n","    # randn is standard normal distribution generator.\n","    w=torch.randn(shape,device=device,dtype=dtype)*np.sqrt(2./fan_in)\n","    w.requires_grad=True # we want to compute gradients for these Tensors\n","                         # during the backward pass\n","    return w"],"metadata":{"id":"Xm_d317xn6CC","executionInfo":{"status":"ok","timestamp":1706954712116,"user_tz":-540,"elapsed":8,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Zero Initialization\n","def zero_weight(shape):\n","    return torch.zeros(shape,device=device,dtype=dtype,requires_grad=True)"],"metadata":{"id":"MhkxfHACrvyL","executionInfo":{"status":"ok","timestamp":1706954725019,"user_tz":-540,"elapsed":824,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["random_weight((3,5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbShq7AwsJVn","executionInfo":{"status":"ok","timestamp":1706954728299,"user_tz":-540,"elapsed":6,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"3cc6e7e3-5d8a-47e4-b0b9-4ecbaf36d61c"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1584,  1.7648, -0.1405,  0.6933, -1.5713],\n","        [ 0.5332, -0.5303, -0.6675,  0.4311, -1.0413],\n","        [-1.3571, -0.2477, -0.0756,  0.1627, -0.9148]], device='cuda:0',\n","       requires_grad=True)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["zero_weight((3,5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLJFXWODsSD6","executionInfo":{"status":"ok","timestamp":1706954728914,"user_tz":-540,"elapsed":4,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"6aab6a68-f628-4b84-b529-83c22c78b7e0"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0.]], device='cuda:0', requires_grad=True)"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["## 2-4.Barebones PyTorch: Check Accuracy\n","모델을 훈련할 때 다음 함수를 사용하여 훈련 또는 검증 세트에서 모델의 정확도를 확인합니다.\n","\n","정확도를 확인할 때 기울기를 계산할 필요가 없으므로 점수를 계산할 때 PyTorch가 계산 그래프를 만들 필요가 없습니다.  \n","그래프가 생성되는 것을 방지하기 위해 `torch.no_grad()` context manager에서 계산 범위를 지정합니다."],"metadata":{"id":"sNg4q_kHsViH"}},{"cell_type":"code","source":["from tqdm import tqdm\n","def check_accuracy_part2(loader,model_fn,params):\n","    \"\"\"\n","    Check the accuracy of a classification model.\n","\n","    Inputs:\n","    - loader: A DataLoader for the data split we want to check\n","    - model_fn: A function that performs the forward pass of the model,\n","      with the signature scores = model_fn(x, params)\n","    - params: List of PyTorch Tensors giving parameters of the model\n","\n","    Returns: Nothing, but prints the accuracy of the model\n","    \"\"\"\n","    split=None\n","    if loader.dataset.train:\n","        split='valid'\n","        if loader.sampler.indices[-1]+1==NUM_TRAIN:\n","            split='train'\n","    else:\n","        split='test'\n","    print('[%s set accuracy]: ' % split,end=' ')\n","\n","    num_correct,num_samples=0,0\n","    with torch.no_grad():\n","        for x,y in tqdm(loader):\n","            x=x.to(device=device,dtype=dtype)\n","            y=y.to(device=device,dtype=torch.int64)\n","            scores=model_fn(x,params)\n","            _,preds=scores.max(1) # row에서 최댓값,인덱스\n","            num_correct+=(preds==y).sum()\n","            num_samples+=preds.size(0)\n","        acc=float(num_correct)/num_samples\n","        print(': %d / %d correct (%.2f%%)' % (num_correct,num_samples,100*acc))"],"metadata":{"id":"u9AVR-h0scjj","executionInfo":{"status":"ok","timestamp":1706954814415,"user_tz":-540,"elapsed":628,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["## 2-5. BareBones PyTorch: Training Loop\n","이제 네트워크를 훈련하기 위한 basic training loop를 설정할 수 있습니다. momentum 없이 Stochastic gradient descent를 사용하여 모델을 훈련할 것입니다. 여기서는 `torch.functional.cross_entropy`를 사용하여 loss를 계산할 것입니다(http://pytorch.org/docs/stable/nn.html#cross-entropy).\n","\n","training loop는 신경망 함수, 초기화된 매개변수 목록(예제에서는 `[w1, w2]`), 학습 속도를 입력으로 받습니다."],"metadata":{"id":"2eQ7HnvSuj51"}},{"cell_type":"code","source":["def train_part2(model_fn,params,learning_rate):\n","    \"\"\"\n","    Train a model on CIFAR-10.\n","\n","    Inputs:\n","    - model_fn: A Python function that performs the forward pass of the model.\n","      It should have the signature scores = model_fn(x, params) where x is a\n","      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n","      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n","      scores for the elements in x.\n","    - params: List of PyTorch Tensors giving weights for the model\n","    - learning_rate: Python scalar giving the learning rate to use for SGD\n","\n","    Returns: Nothing\n","    \"\"\"\n","    for batch_idx,(x,y) in enumerate(loader_train):\n","        # Move the data to the proper device (GPU or CPU)\n","        x=x.to(device=device,dtype=dtype)\n","        y=y.to(device=device,dtype=torch.long)\n","\n","        # Forward pass: compute scores and loss\n","        scores=model_fn(x,params)\n","        loss=F.cross_entropy(scores,y)\n","\n","        # Backward pass:\n","        ## PyTorch figures out which Tensors in the computational\n","        ## graph has requires_grad=True and uses backpropagation to compute the\n","        ## gradient of the loss with respect to these Tensors, and stores the\n","        ## gradients in the .grad attribute of each Tensor.\n","        loss.backward()\n","\n","        # Update parameters:\n","        ## We don't want to backpropagate through the\n","        ## parameter updates, so we scope the updates under a torch.no_grad()\n","        ## context manager to prevent a computational graph from being built.\n","        with torch.no_grad():\n","            for w in params:\n","                w-=learning_rate*w.grad\n","                w.grad.zero_()  # Manually zero the gradients\n","\n","        if batch_idx%100==0:\n","            print('Iteration %d, loss = %.4f'%(batch_idx,loss.item()))\n","            check_accuracy_part2(loader_val,model_fn,params)\n","            check_accuracy_part2(loader_train,model_fn,params)\n","            print()"],"metadata":{"id":"QJkv4WSouq8s","executionInfo":{"status":"ok","timestamp":1706954826715,"user_tz":-540,"elapsed":529,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["## 2-6. Barebones PyTorch: Train a Two-Layer Network"],"metadata":{"id":"ns50_BkOdCIV"}},{"cell_type":"code","source":["hidden_layer_size = 4000\n","learning_rate = 1e-2\n","\n","w1 = random_weight((3 * 32 * 32, hidden_layer_size))\n","w2 = random_weight((hidden_layer_size, 10))\n","\n","train_part2(two_layer_fc, [w1, w2], learning_rate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c2rcdvlRxc7o","executionInfo":{"status":"ok","timestamp":1706954949004,"user_tz":-540,"elapsed":120699,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"da8f7025-f3fb-4bc6-eb45-3b6d6029f522"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0, loss = 3.5746\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 51.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 153 / 1000 correct (15.30%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:12<00:00, 59.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 7109 / 49000 correct (14.51%)\n","\n","Iteration 100, loss = 1.8909\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 65.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 282 / 1000 correct (28.20%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:12<00:00, 59.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 14957 / 49000 correct (30.52%)\n","\n","Iteration 200, loss = 2.3340\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 67.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 376 / 1000 correct (37.60%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 58.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 18554 / 49000 correct (37.87%)\n","\n","Iteration 300, loss = 1.8472\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 61.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 409 / 1000 correct (40.90%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:12<00:00, 59.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 20769 / 49000 correct (42.39%)\n","\n","Iteration 400, loss = 1.8078\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 65.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 415 / 1000 correct (41.50%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 57.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 21594 / 49000 correct (44.07%)\n","\n","Iteration 500, loss = 1.9854\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 45.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 367 / 1000 correct (36.70%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 57.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 19401 / 49000 correct (39.59%)\n","\n","Iteration 600, loss = 1.8012\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 62.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 443 / 1000 correct (44.30%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 57.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 23763 / 49000 correct (48.50%)\n","\n","Iteration 700, loss = 1.7893\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 64.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 445 / 1000 correct (44.50%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 58.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 23695 / 49000 correct (48.36%)\n","\n"]}]},{"cell_type":"markdown","source":["## 2-7. BareBones PyTorch: Training a ConvNet\n","아래에서는 위에서 정의한 함수를 사용하여 CIFAR에서 3계층 Convolutional network를 훈련해야 합니다. 네트워크의 아키텍처는 다음과 같아야 합니다:\n","\n","1. 32개의 5x5 필터가 있는 컨볼루션 레이어(바이어스 포함), 제로 패딩은 2입니다.\n","2. ReLU\n","3. 16개의 3x3 필터가 있는 컨볼루션 레이어(바이어스 포함), 제로 패딩 1\n","4. ReLU\n","5. 10개의 클래스에 대한 점수를 계산하기 위한 완전 연결 레이어(바이어스 포함)\n","\n","위에서 정의한 `random_weight` 함수를 사용하여 가중치 행렬을 초기화해야 하며, 위의 `zero_weight` 함수를 사용하여 바이어스 벡터를 초기화해야 합니다.\n","\n","하이퍼파라미터를 조정할 필요는 없지만 모든 것이 올바르게 작동하면 한 에포크 후에 42% 이상의 정확도를 달성해야 합니다."],"metadata":{"id":"xG0CF7lL1w1A"}},{"cell_type":"code","source":["'''\n","    Inputs:\n","    - x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images\n","    - params: A list of PyTorch Tensors giving the weights and biases for the\n","      network; should contain the following:\n","      - conv_w1: PyTorch Tensor of shape (channel_1, 3, KH1, KW1) giving weights\n","        for the first convolutional layer\n","      - conv_b1: PyTorch Tensor of shape (channel_1,) giving biases for the first\n","        convolutional layer\n","      - conv_w2: PyTorch Tensor of shape (channel_2, channel_1, KH2, KW2) giving\n","        weights for the second convolutional layer\n","      - conv_b2: PyTorch Tensor of shape (channel_2,) giving biases for the second\n","        convolutional layer\n","      - fc_w: PyTorch Tensor giving weights for the fully-connected layer. Can you\n","        figure out what the shape should be?\n","      - fc_b: PyTorch Tensor giving biases for the fully-connected layer. Can you\n","        figure out what the shape should be?\n","'''\n","\n","learning_rate = 3e-3\n","\n","channel_1 = 32\n","channel_2 = 16\n","\n","conv_w1 = random_weight((channel_1,3,5,5)) # [out_channel, in_channel, KW,KH]\n","conv_b1 = zero_weight((channel_1,))\n","conv_w2 = random_weight((channel_2,channel_1,3,3))\n","conv_b2 = zero_weight((channel_2,))\n","fc_w = random_weight((32*32*channel_2,10))\n","fc_b = zero_weight((10,))"],"metadata":{"id":"y_QK1XLcdRxG","executionInfo":{"status":"ok","timestamp":1706956060493,"user_tz":-540,"elapsed":7,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n","train_part2(three_layer_convnet, params, learning_rate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_q1_pz1lf1ZC","executionInfo":{"status":"ok","timestamp":1706956190805,"user_tz":-540,"elapsed":126299,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"b54994a5-ed18-46d7-cbe1-75034f97161c"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0, loss = 4.0463\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 60.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 138 / 1000 correct (13.80%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 56.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 6543 / 49000 correct (13.35%)\n","\n","Iteration 100, loss = 2.0432\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 36.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 368 / 1000 correct (36.80%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 57.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 17472 / 49000 correct (35.66%)\n","\n","Iteration 200, loss = 1.8334\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 43.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 408 / 1000 correct (40.80%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 56.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 20050 / 49000 correct (40.92%)\n","\n","Iteration 300, loss = 1.5649\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 64.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 437 / 1000 correct (43.70%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 58.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 21109 / 49000 correct (43.08%)\n","\n","Iteration 400, loss = 1.5894\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 59.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 464 / 1000 correct (46.40%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 56.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 22019 / 49000 correct (44.94%)\n","\n","Iteration 500, loss = 1.3616\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 60.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 488 / 1000 correct (48.80%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 55.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 23389 / 49000 correct (47.73%)\n","\n","Iteration 600, loss = 1.2573\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 66.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 509 / 1000 correct (50.90%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:13<00:00, 57.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 24093 / 49000 correct (49.17%)\n","\n","Iteration 700, loss = 1.3751\n","[valid set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [00:00<00:00, 44.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 513 / 1000 correct (51.30%)\n","[train set accuracy]:  "]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 766/766 [00:14<00:00, 54.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":[": 24660 / 49000 correct (50.33%)\n","\n"]}]},{"cell_type":"markdown","source":["# Part3. PyTorch Module API"],"metadata":{"id":"yHyLEj2QgoKX"}},{"cell_type":"markdown","source":["Barebone PyTorch에서는 모든 Parameter tensors를 수작업으로 추적해야 합니다. 이는 몇 개의 텐서가 있는 소규모 네트워크에서는 괜찮지만, 대규모 네트워크에서 수십 또는 수백 개의 텐서를 추적하는 것은 매우 불편하고 오류가 발생하기 쉽습니다.\n","\n","PyTorch는 임의의 네트워크 아키텍처를 정의하는 동시에 학습 가능한 모든 파라미터를 추적할 수 있도록 `nn.Module` API를 제공합니다. Part II에서는 SGD를 직접 구현해 보았습니다. PyTorch는 또한 RMSProp, Adagrad, Adam과 같은 모든 일반적인 Optimizer를 구현하는 `torch.optim` 패키지를 제공합니다. 심지어 L-BFGS와 같은 대략적인 2차 방법도 지원합니다! 각 옵티마이저의 정확한 사양은 [문서](http://pytorch.org/docs/master/optim.html)를 참고하세요.\n","\n","모듈 API를 사용하려면 아래 단계를 따르세요:\n","\n","1. 서브클래스 `nn.Module`. 네트워크 클래스에 `TwoLayerFC`와 같은 직관적인 이름을 지정합니다.\n","\n","2. 생성자 `__init__()`에서 필요한 모든 레이어를 클래스 속성으로 정의합니다. `nn.Linear` 및 `nn.Conv2d`와 같은 레이어 객체는 그 자체로 `nn.Module` 서브클래스이며 학습 가능한 파라미터를 포함하므로 원시 텐서를 직접 인스턴스화할 필요가 없습니다. `nn.Module`이 이러한 내부 파라미터를 추적합니다. 수십 개의 내장 레이어에 대해 자세히 알아보려면 [문서](http://pytorch.org/docs/master/nn.html)를 참조하세요. **경고**: `super().__init__()`를 먼저 호출하는 것을 잊지 마세요!\n","\n","3. `forward()` 메서드에서 네트워크의 *연결성*을 정의합니다. 텐서를 입력으로 받고 \"변환된\" 텐서를 출력하는 함수 호출로 `__init__`에 정의된 속성을 사용해야 합니다. `forward()`에서 학습 가능한 매개변수가 있는 새 레이어를 생성하지 마세요! 모든 매개변수는 `__init__`에서 미리 선언해야 합니다.\n","\n","모듈 서브클래스를 정의한 후에는 객체로 인스턴스화하여 Part II의 NN 전달 함수처럼 호출할 수 있습니다."],"metadata":{"id":"V66zYIWzlOEC"}},{"cell_type":"markdown","source":["## 3-1. Module API: Two-Layer Network"],"metadata":{"id":"fCC0y264l8rc"}},{"cell_type":"code","source":["# Fully connectd 2계층 네트워크\n","class TwoLayerFC(nn.Module):\n","    def __init__(self, input_size,hidden_size,num_classes):\n","        super().__init__()\n","\n","        self.fc1=nn.Linear(input_size,hidden_size)\n","        nn.init.kaiming_normal_(self.fc1.weight)\n","\n","        self.fc2=nn.Linear(hidden_size,num_classes)\n","        nn.init.kaiming_normal_(self.fc2.weight)\n","\n","    def forward(self,x):\n","        x=flatten(x) # x=x.veiw(x.size(0),-1)\n","        scores=self.fc2(F.relu(self.fc1(x)))\n","        return scores"],"metadata":{"id":"hIr1HvaNmEuG","executionInfo":{"status":"ok","timestamp":1706958217646,"user_tz":-540,"elapsed":3,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["def test_TwoLayerFC():\n","    input_size=50\n","    x=torch.zeros((64,input_size),dtype=dtype) # minibatch size 64, feature dimension 50\n","    model=TwoLayerFC(input_size=input_size,hidden_size=42,num_classes=10)\n","    scores=model(x)\n","    print(scores[0])\n","    print(scores.size())"],"metadata":{"id":"S0tL3x51oNnW","executionInfo":{"status":"ok","timestamp":1706958237903,"user_tz":-540,"elapsed":8,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["test_TwoLayerFC()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNgudMiYowxL","executionInfo":{"status":"ok","timestamp":1706958240235,"user_tz":-540,"elapsed":9,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"67ed7fc6-1212-47bb-97b8-50bbedbe94cc"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.0349, -0.0183,  0.1291, -0.2349,  0.0035, -0.0112,  0.2927, -0.0782,\n","         0.1563,  0.0135], grad_fn=<SelectBackward0>)\n","torch.Size([64, 10])\n"]}]},{"cell_type":"markdown","source":["## 3-2. Module API: Three-Layer ConvNet\n","이제 완전히 연결된 레이어에 이어 3계층 ConvNet을 구현할 차례입니다. 네트워크 아키텍처는 파트 II와 동일해야 합니다:\n","\n","1. zero-padding이 2인 `channel_1` 5x5 필터가 있는 컨볼루션 레이어\n","2. ReLU\n","3. `channel_2` 3x3 필터가 있는 Convolution Layer, zero-padding 1\n","4. ReLU\n","5. `num_classes` 클래스에 완전히 연결된 레이어\n","\n","Kaiming normal initialize 방법을 사용하여 모델의 가중치 행렬을 초기화해야 합니다.\n","\n","**HINT**: http://pytorch.org/docs/stable/nn.html#conv2d\n","\n","3계층 ConvNet을 구현한 후 `test_ThreeLayerConvNet` 함수가 구현을 실행하면 출력 점수의 모양에 대해 `(64, 10)`이 출력되어야 합니다.  \n","  \n","Fully-connected layer 이후에는 소프트맥스 활성화가 없음에 유의하십시오: 이는 PyTorch의 교차 엔트로피 손실이 소프트맥스 활성화를 수행하기 때문이며, 이 단계를 번들로 묶으면 계산이 더 효율적이기 때문입니다."],"metadata":{"id":"qy25wDK0oybX"}},{"cell_type":"code","source":["class ThreeLayerConvNet(nn.Module):\n","    def __init__(self, in_channel, channel_1, channel_2,num_classes):\n","        super().__init__()\n","\n","        self.conv1=nn.Conv2d(in_channel,channel_1,5,padding=2) # in_channels, out_channels, kernel_size\n","        nn.init.kaiming_normal_(self.conv1.weight)\n","        self.conv2=nn.Conv2d(channel_1,channel_2,3,padding=1)\n","        nn.init.kaiming_normal_(self.conv2.weight)\n","        self.fc=nn.Linear(channel_2*32*32,num_classes)\n","        nn.init.kaiming_normal_(self.fc.weight)\n","\n","    def forward(self,x):\n","        x=F.relu(self.conv1(x))\n","        x=F.relu(self.conv2(x))\n","        x=flatten(x)\n","        scores=self.fc(x)\n","        return scores"],"metadata":{"id":"mW-0SOTbpXr3","executionInfo":{"status":"ok","timestamp":1706959184204,"user_tz":-540,"elapsed":3,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["def test_ThreeLayerConvNet():\n","    x=torch.zeros((64,3,32,32),dtype=dtype)\n","    model=ThreeLayerConvNet(in_channel=3, channel_1=12, channel_2=8, num_classes=10)\n","    scores=model(x)\n","    print(scores.size())\n","\n","test_ThreeLayerConvNet()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUd6DHiJqNt2","executionInfo":{"status":"ok","timestamp":1706959186340,"user_tz":-540,"elapsed":2,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"f7aa668f-3b6e-4c72-80ce-77253bff2b16"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 10])\n"]}]},{"cell_type":"markdown","source":["## 3-3. Module API: Check Accuracy"],"metadata":{"id":"Rb2_ILEascJK"}},{"cell_type":"code","source":["def check_accuracy_part34(loader,model):\n","    split=None\n","    if loader.dataset.train:\n","        if loader.sampler.indices[-1]+1==NUM_TRAIN: split='Train'\n","        else: split='Valid'\n","    else: split='Test'\n","\n","    print('%s set accuracy:' % split,end=' ')\n","    num_correct=0\n","    num_samples=0\n","\n","    model.eval() # set model to evaluation mode\n","    with torch.no_grad():\n","        for x,y in loader:\n","            x=x.to(device=device,dtype=dtype)\n","            y=y.to(device=device,dtype=torch.long)\n","            scores=model(x)\n","            _,preds=scores.max(1)\n","            num_correct+=(preds==y).sum()\n","            num_samples+=preds.size(0)\n","        acc=float(num_correct)/num_samples\n","        print('Got %d / %d correct (%.2f)' % (num_correct,num_samples,100*acc))\n"],"metadata":{"id":"qustSBL4sn2J","executionInfo":{"status":"ok","timestamp":1706961205113,"user_tz":-540,"elapsed":3,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":["## 3-4. Module API: Training Loop\n","또한 약간 다른 training loop를 사용합니다. 가중치 값을 직접 업데이트하는 대신, Optimization 알고리즘의 개념을 추상화하고 신경망 최적화에 일반적으로 사용되는 대부분의 알고리즘 구현을 제공하는 `torch.optim` 패키지의 Optimizer 객체를 사용합니다."],"metadata":{"id":"oEaMXpr-vRTg"}},{"cell_type":"code","source":["def train_part34(model, optimizer, epochs=1):\n","    \"\"\"\n","    Train a model on CIFAR-10 using the PyTorch Module API.\n","\n","    Inputs:\n","    - model: A PyTorch Module giving the model to train.\n","    - optimizer: An Optimizer object we will use to train the model\n","    - epochs: (Optional) A Python integer giving the number of epochs to train for\n","\n","    Returns: Nothing, but prints model accuracies during training.\n","    \"\"\"\n","    model=model.to(device=device)\n","    for e in range(epochs):\n","        for batch_idx,(x,y) in enumerate(loader_train):\n","            model.train()\n","            x=x.to(device=device,dtype=dtype)\n","            y=y.to(device=device,dtype=torch.long)\n","\n","            scores=model(x)\n","            loss=F.cross_entropy(scores,y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch_idx%100==0:\n","                print('Iteration %d, loss= %.4f' % (batch_idx,loss.item()))\n","                check_accuracy_part34(loader_train,model)\n","                check_accuracy_part34(loader_val,model)\n","                print()"],"metadata":{"id":"G0kDU3qQxSMN","executionInfo":{"status":"ok","timestamp":1706961208454,"user_tz":-540,"elapsed":4,"user":{"displayName":"노지예","userId":"16611815830350888933"}}},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":["## 3-5. Module API: Train a Two-Layer Network\n","\n","이제 training loop를 실행할 준비가 되었습니다. Part II와 달리 이번에는 Parameter tensors를 더 이상 명시적으로 할당하지 않습니다.\n","\n","입력 크기, 숨겨진 레이어 크기, 클래스 수(즉, 출력 크기)를 `TwoLayerFC`의 생성자에 전달하기만 하면 됩니다.\n","\n","또한 `TwoLayerFC` 내에서 학습 가능한 모든 파라미터를 추적하는 옵티마이저를 정의해야 합니다.\n","\n","하이퍼파라미터를 조정할 필요는 없지만, 한 epoch 동안 학습한 후 40% 이상의 모델 정확도를 볼 수 있어야 합니다."],"metadata":{"id":"SM5-omxOzBMx"}},{"cell_type":"code","source":["hidden_layer_size=4000\n","learning_rate=1e-2\n","model=TwoLayerFC(3*32*32,hidden_layer_size,10)\n","optimizer=optim.SGD(model.parameters(),lr=learning_rate)\n","\n","train_part34(model,optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hozAu5jvzLLR","executionInfo":{"status":"ok","timestamp":1706961328187,"user_tz":-540,"elapsed":116443,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"b451b903-9a02-4b06-958d-551156f7ba1c"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0, loss= 3.7868\n","Train set accuracy: Got 6640 / 49000 correct (13.55)\n","Valid set accuracy: Got 125 / 1000 correct (12.50)\n","\n","Iteration 100, loss= 2.4927\n","Train set accuracy: Got 16397 / 49000 correct (33.46)\n","Valid set accuracy: Got 346 / 1000 correct (34.60)\n","\n","Iteration 200, loss= 2.4485\n","Train set accuracy: Got 19800 / 49000 correct (40.41)\n","Valid set accuracy: Got 392 / 1000 correct (39.20)\n","\n","Iteration 300, loss= 2.1912\n","Train set accuracy: Got 19559 / 49000 correct (39.92)\n","Valid set accuracy: Got 371 / 1000 correct (37.10)\n","\n","Iteration 400, loss= 1.6511\n","Train set accuracy: Got 20889 / 49000 correct (42.63)\n","Valid set accuracy: Got 408 / 1000 correct (40.80)\n","\n","Iteration 500, loss= 1.6130\n","Train set accuracy: Got 22238 / 49000 correct (45.38)\n","Valid set accuracy: Got 405 / 1000 correct (40.50)\n","\n","Iteration 600, loss= 1.8589\n","Train set accuracy: Got 23538 / 49000 correct (48.04)\n","Valid set accuracy: Got 439 / 1000 correct (43.90)\n","\n","Iteration 700, loss= 1.4658\n","Train set accuracy: Got 24794 / 49000 correct (50.60)\n","Valid set accuracy: Got 479 / 1000 correct (47.90)\n","\n"]}]},{"cell_type":"markdown","source":["## Module API: Train a Three-Layer ConvNet"],"metadata":{"id":"4ukLTbd6zo0P"}},{"cell_type":"markdown","source":["이제 Module API를 사용하여 CIFAR에서 3계층 ConvNet을 훈련해야 합니다. 이는 2계층 네트워크 훈련과 매우 유사하게 보일 것입니다! 하이퍼파라미터를 조정할 필요는 없지만, 한 회기 동안 훈련한 후 45% 이상을 달성해야 합니다.\n","\n","Momentum이 없는 Stochastic gradient descent를 사용하여 모델을 훈련해야 합니다."],"metadata":{"id":"iiCR78nb1QSg"}},{"cell_type":"code","source":["learning_rate=3e-3\n","channel_1=32\n","channel_2=16\n","\n","model=ThreeLayerConvNet(3,channel_1,channel_2,10)\n","optimizer=optim.SGD(model.parameters(),lr=learning_rate)\n","\n","train_part34(model,optimizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tzN53YD1b4t","executionInfo":{"status":"ok","timestamp":1706961803026,"user_tz":-540,"elapsed":115387,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"29879905-1c58-4c8b-ebd5-8547eb252854"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0, loss= 4.1160\n","Train set accuracy: Got 5332 / 49000 correct (10.88)\n","Valid set accuracy: Got 102 / 1000 correct (10.20)\n","\n","Iteration 100, loss= 1.7934\n","Train set accuracy: Got 17381 / 49000 correct (35.47)\n","Valid set accuracy: Got 361 / 1000 correct (36.10)\n","\n","Iteration 200, loss= 1.7038\n","Train set accuracy: Got 19515 / 49000 correct (39.83)\n","Valid set accuracy: Got 396 / 1000 correct (39.60)\n","\n","Iteration 300, loss= 1.6368\n","Train set accuracy: Got 21039 / 49000 correct (42.94)\n","Valid set accuracy: Got 423 / 1000 correct (42.30)\n","\n","Iteration 400, loss= 1.6731\n","Train set accuracy: Got 21814 / 49000 correct (44.52)\n","Valid set accuracy: Got 433 / 1000 correct (43.30)\n","\n","Iteration 500, loss= 1.6021\n","Train set accuracy: Got 22841 / 49000 correct (46.61)\n","Valid set accuracy: Got 444 / 1000 correct (44.40)\n","\n","Iteration 600, loss= 1.4531\n","Train set accuracy: Got 23248 / 49000 correct (47.44)\n","Valid set accuracy: Got 460 / 1000 correct (46.00)\n","\n","Iteration 700, loss= 1.6988\n","Train set accuracy: Got 23767 / 49000 correct (48.50)\n","Valid set accuracy: Got 451 / 1000 correct (45.10)\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eOhLSXJp2Ert"},"execution_count":null,"outputs":[]}]}